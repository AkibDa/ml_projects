{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading all necessary libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading necessary NLTK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/skakibahammed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/skakibahammed/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>news_url</th>\n",
       "      <th>source_domain</th>\n",
       "      <th>tweet_num</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kandi Burruss Explodes Over Rape Accusation on...</td>\n",
       "      <td>http://toofab.com/2017/05/08/real-housewives-a...</td>\n",
       "      <td>toofab.com</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>People's Choice Awards 2018: The best red carp...</td>\n",
       "      <td>https://www.today.com/style/see-people-s-choic...</td>\n",
       "      <td>www.today.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sophia Bush Sends Sweet Birthday Message to 'O...</td>\n",
       "      <td>https://www.etonline.com/news/220806_sophia_bu...</td>\n",
       "      <td>www.etonline.com</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Colombian singer Maluma sparks rumours of inap...</td>\n",
       "      <td>https://www.dailymail.co.uk/news/article-33655...</td>\n",
       "      <td>www.dailymail.co.uk</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gossip Girl 10 Years Later: How Upper East Sid...</td>\n",
       "      <td>https://www.zerchoo.com/entertainment/gossip-g...</td>\n",
       "      <td>www.zerchoo.com</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Kandi Burruss Explodes Over Rape Accusation on...   \n",
       "1  People's Choice Awards 2018: The best red carp...   \n",
       "2  Sophia Bush Sends Sweet Birthday Message to 'O...   \n",
       "3  Colombian singer Maluma sparks rumours of inap...   \n",
       "4  Gossip Girl 10 Years Later: How Upper East Sid...   \n",
       "\n",
       "                                            news_url        source_domain  \\\n",
       "0  http://toofab.com/2017/05/08/real-housewives-a...           toofab.com   \n",
       "1  https://www.today.com/style/see-people-s-choic...        www.today.com   \n",
       "2  https://www.etonline.com/news/220806_sophia_bu...     www.etonline.com   \n",
       "3  https://www.dailymail.co.uk/news/article-33655...  www.dailymail.co.uk   \n",
       "4  https://www.zerchoo.com/entertainment/gossip-g...      www.zerchoo.com   \n",
       "\n",
       "   tweet_num  real  \n",
       "0         42     1  \n",
       "1          0     1  \n",
       "2         63     1  \n",
       "3         20     1  \n",
       "4         38     1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('FakeNewsNet.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([\"news_url\", \"tweet_num\"], axis=1)\n",
    "data['source_domain'] = data['source_domain'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = data[data['real'] == 1]\n",
    "fake_data = data[data['real'] == 0]\n",
    "real_sample = real_data.sample(n=len(fake_data), random_state=42)\n",
    "balanced_data = pd.concat([real_sample, fake_data])\n",
    "\n",
    "balanced_data = balanced_data.drop(['source_domain'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(text):\n",
    "  text = text.lower()\n",
    "  text = nltk.word_tokenize(text)\n",
    "  y = [i for i in text if i.isalnum()]\n",
    "  text = [i for i in y if i not in nltk.corpus.stopwords.words('english') and i not in string.punctuation]\n",
    "  y = [nltk.stem.PorterStemmer().stem(i) for i in text]\n",
    "  return \" \".join(y)\n",
    "\n",
    "balanced_data['transformed_text'] = balanced_data['title'].apply(transform_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "preprocessor = ColumnTransformer(transformers=[(\"text\", tfidf, 'transformed_text')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = balanced_data[['transformed_text']]\n",
    "y = balanced_data['real'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "  \"Logistic Regression\": LogisticRegression(),\n",
    "  \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "  \"Support Vector Machine\": SVC(),\n",
    "  \"Decision Tree\": DecisionTreeClassifier(),\n",
    "  \"Random Forest\": RandomForestClassifier(),\n",
    "  \"XGBoost\": XGBClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grids = {\n",
    "  \"Logistic Regression\": {\n",
    "    'model__C': [0.1, 1.0, 10],\n",
    "    'model__solver': ['liblinear']\n",
    "  },\n",
    "  \"K-Nearest Neighbors\": {\n",
    "    'model__n_neighbors': [5, 7, 9],\n",
    "    'model__weights': ['uniform', 'distance']\n",
    "  },\n",
    "  \"Support Vector Machine\": {\n",
    "    'model__C': [0.1, 1.0, 10],\n",
    "    'model__gamma': ['scale', 'auto'],\n",
    "    'model__kernel': ['rbf']\n",
    "  },\n",
    "  \"Decision Tree\": {\n",
    "    'model__max_depth': [10, 20, None],\n",
    "    'model__min_samples_split': [2, 5, 10]\n",
    "  },\n",
    "  \"Random Forest\": {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [10, 20, None]\n",
    "  },\n",
    "  \"XGBoost\": {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__learning_rate': [0.05, 0.1],\n",
    "    'model__max_depth': [3, 5, 7]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for Logistic Regression...\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Running GridSearchCV for K-Nearest Neighbors...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Running GridSearchCV for Support Vector Machine...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Running GridSearchCV for Decision Tree...\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Running GridSearchCV for Random Forest...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Running GridSearchCV for XGBoost...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "\n",
    "for name, model in models.items():\n",
    "  pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", model)\n",
    "  ])\n",
    "\n",
    "  param_grid = param_grids[name]\n",
    "\n",
    "  print(f\"Running GridSearchCV for {name}...\")\n",
    "  grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    "  )\n",
    "  \n",
    "  grid_search.fit(X_train, Y_train)\n",
    "\n",
    "  best_model = grid_search.best_estimator_\n",
    "  \n",
    "  Y_pred = best_model.predict(X_test)\n",
    "  \n",
    "  accuracy = accuracy_score(Y_test, Y_pred)\n",
    "  precision = precision_score(Y_test, Y_pred)\n",
    "  recall = recall_score(Y_test, Y_pred)\n",
    "  f1 = f1_score(Y_test, Y_pred)\n",
    "  \n",
    "  results_list.append({\n",
    "    \"Model\": name,\n",
    "    \"Accuracy\": accuracy,\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1-Score\": f1,\n",
    "    \"Best Params\": grid_search.best_params_\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Model Performance after GridSearchCV:\n",
      "                    Model  Accuracy  Precision  Recall  F1-Score  \\\n",
      "0     Logistic Regression    0.7750     0.7567  0.8106    0.7827   \n",
      "1     K-Nearest Neighbors    0.6151     0.6063  0.6568    0.6305   \n",
      "2  Support Vector Machine    0.7815     0.7576  0.8280    0.7912   \n",
      "3           Decision Tree    0.6877     0.6388  0.8636    0.7344   \n",
      "4           Random Forest    0.7268     0.6810  0.8532    0.7574   \n",
      "5                 XGBoost    0.7372     0.7062  0.8123    0.7556   \n",
      "\n",
      "                                         Best Params  \n",
      "0    {'model__C': 1.0, 'model__solver': 'liblinear'}  \n",
      "1  {'model__n_neighbors': 7, 'model__weights': 'd...  \n",
      "2  {'model__C': 1.0, 'model__gamma': 'scale', 'mo...  \n",
      "3  {'model__max_depth': 20, 'model__min_samples_s...  \n",
      "4  {'model__max_depth': 20, 'model__n_estimators'...  \n",
      "5  {'model__learning_rate': 0.1, 'model__max_dept...  \n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results_list)\n",
    "results_df = results_df.round(4)\n",
    "print(\"\\nFinal Model Performance after GridSearchCV:\")\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
